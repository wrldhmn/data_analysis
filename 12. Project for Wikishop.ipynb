{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project for Wikishop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikishop online store is launching a new service. Now users can edit and add product descriptions, just like in wiki communities. That is, clients propose their own edits and comment on the changes of others. The store needs a tool that will look for toxic comments and send them for moderation.\n",
    "\n",
    "Let's train the model to classify comments into positive and negative. We have a data set with markings about the toxicity of edits.\n",
    "\n",
    "Let's build a model with a quality metric value *F1* of at least 0.75.\n",
    "\n",
    "**Instructions for completing the project**\n",
    "\n",
    "1. Download and prepare the data.\n",
    "2. Let's train different models.\n",
    "3. Let's draw conclusions.\n",
    "\n",
    "It is not necessary to use *BERT* to complete the project, but you can try.\n",
    "\n",
    "**Description of data**\n",
    "\n",
    "The data is in the file `toxic_comments.csv`.\n",
    "\n",
    "The *text* column in it contains the text of the comment, and *toxic* is the target attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries and take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the lemmatization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "df['lemma_text'] = df['text'].apply(lemmatize_text)\n",
    "df['lemma_text'] = df['lemma_text'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove extra characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    return \" \".join(text.split())\n",
    "df['lemma_text'] = df['lemma_text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide the data into test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79785, 3)\n",
      "(79786, 3)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size = 0.5, random_state = 12345)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train['lemma_text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(sublinear_tf = True, stop_words=stopwords)\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = train['toxic'].values.astype('U')\n",
    "target_test = test['toxic'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = count_tf_idf.transform(test['lemma_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 для Logistic Regression (регуляризация l1) на тестовой выборке: 0.7711739527050733\n",
      "\n",
      "F1 для Logistic Regression (регуляризация l2) на тестовой выборке: 0.7132268663559257\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state = 12345)\n",
    "parameters_lr_1 = {'penalty':['l1'],\n",
    "                 'solver':['liblinear', 'saga'],\n",
    "                 'class_weight':[None, 'balanced']}\n",
    "parameters_lr_2 = {'penalty':['l2'],\n",
    "                 'solver':['lbfgs', 'saga', 'sag', 'newton-cg'], \n",
    "                 'class_weight':[None, 'balanced']}\n",
    "\n",
    "search_lr_1 = RandomizedSearchCV(lr, parameters_lr_1, cv=5, n_jobs = -1)\n",
    "search_lr_1.fit(tf_idf, target_train)\n",
    "best_lr_1 = search_lr_1.best_estimator_\n",
    "predict_lr_1 = best_lr_1.predict(features_test)\n",
    "\n",
    "search_lr_2 = RandomizedSearchCV(lr, parameters_lr_2, cv=5, n_jobs = -1)\n",
    "search_lr_2.fit(tf_idf, target_train)\n",
    "best_lr_2 = search_lr_2.best_estimator_\n",
    "predict_lr_2 = best_lr_2.predict(features_test)\n",
    "\n",
    "print('F1 для Logistic Regression (регуляризация l1) на тестовой выборке:', f1_score(target_test, predict_lr_1, pos_label='1'))\n",
    "print()\n",
    "print('F1 для Logistic Regression (регуляризация l2) на тестовой выборке:', f1_score(target_test, predict_lr_2, pos_label='1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SGD</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 для SGD на тестовой выборке: 0.7035253921416369\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(random_state = 12345)\n",
    "parameters_sgd = {'penalty':['l1', 'l2', 'elasticnet'],\n",
    "                  'alpha':[0.00001, 0.00005, 0.0001, 0.001, 0.01, 0.1],\n",
    "                  'max_iter':range(100, 1500, 200),\n",
    "                  'early_stopping': [True, False],\n",
    "                  'class_weight':[None, 'balanced']}\n",
    "search_sgd = RandomizedSearchCV(sgd, parameters_sgd, cv=5, n_jobs = -1)\n",
    "search_sgd.fit(tf_idf, target_train)\n",
    "best_sgd = search_sgd.best_estimator_\n",
    "predict_sgd = best_sgd.predict(features_test)\n",
    "print('F1 для SGD на тестовой выборке:', f1_score(target_test, predict_sgd, pos_label='1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is more suitable for our task. Using it we get F1 = 0.77"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2567,
    "start_time": "2022-05-17T08:51:58.226Z"
   },
   {
    "duration": 118,
    "start_time": "2022-05-17T08:53:21.019Z"
   },
   {
    "duration": 34707,
    "start_time": "2022-05-17T08:53:33.013Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-17T08:54:18.264Z"
   },
   {
    "duration": 965,
    "start_time": "2022-05-17T08:54:27.681Z"
   },
   {
    "duration": 781,
    "start_time": "2022-05-17T08:54:44.339Z"
   },
   {
    "duration": 33578,
    "start_time": "2022-05-17T08:54:49.131Z"
   },
   {
    "duration": 4062,
    "start_time": "2022-05-17T08:55:22.711Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-17T08:57:57.805Z"
   },
   {
    "duration": 59,
    "start_time": "2022-05-17T08:58:04.764Z"
   },
   {
    "duration": 1011,
    "start_time": "2022-05-17T08:58:19.239Z"
   },
   {
    "duration": 4589,
    "start_time": "2022-05-17T08:58:57.325Z"
   },
   {
    "duration": 63,
    "start_time": "2022-05-17T08:59:08.372Z"
   },
   {
    "duration": 3230,
    "start_time": "2022-05-17T08:59:21.023Z"
   },
   {
    "duration": 4109,
    "start_time": "2022-05-17T09:33:34.253Z"
   },
   {
    "duration": 4619,
    "start_time": "2022-05-17T09:42:12.071Z"
   },
   {
    "duration": 74072,
    "start_time": "2022-05-17T09:42:19.102Z"
   },
   {
    "duration": 53108,
    "start_time": "2022-05-17T09:50:48.351Z"
   },
   {
    "duration": 1304,
    "start_time": "2022-05-17T09:53:28.052Z"
   },
   {
    "duration": 11817,
    "start_time": "2022-05-17T09:53:30.751Z"
   },
   {
    "duration": 4771,
    "start_time": "2022-05-17T09:53:42.570Z"
   },
   {
    "duration": 64,
    "start_time": "2022-05-17T09:53:47.343Z"
   },
   {
    "duration": 3167,
    "start_time": "2022-05-17T09:53:47.409Z"
   },
   {
    "duration": 1107175,
    "start_time": "2022-05-17T09:53:53.743Z"
   },
   {
    "duration": 27414,
    "start_time": "2022-05-17T10:38:38.031Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
